#!/usr/bin/env python3
"""
Display information about the Wild Edible Plant Classifier project
"""
import torch
import os
from pathlib import Path

print("=" * 80)
print(" " * 20 + "WILD EDIBLE PLANT CLASSIFIER")
print("=" * 80)

# System Information
print("\nğŸ“Š SYSTEM INFORMATION")
print("â”€" * 80)
print(f"PyTorch Version: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA Version: {torch.version.cuda}")
    print(f"GPU Device: {torch.cuda.get_device_name(0)}")
else:
    print("Device: CPU (MPS backend available on Apple Silicon)")

# Project Overview
print("\nğŸ“š PROJECT OVERVIEW")
print("â”€" * 80)
print("""
This project implements a deep learning classifier for identifying 35 species of
wild edible plants using transfer learning with three state-of-the-art CNN 
architectures:

  â€¢ MobileNet v2  - Lightweight, mobile-optimized architecture
  â€¢ GoogLeNet     - Inception-based architecture with multiple scales
  â€¢ ResNet-34     - Residual network with skip connections

The models were trained on 16,535 images collected from Flickr API, with 400-500
images per plant species.
""")

# Dataset Information
print("\nğŸŒ¿ PLANT SPECIES (35 classes)")
print("â”€" * 80)

plants = [
    "Alfalfa", "Allium", "Borage", "Burdock", "Calendula", "Cattail",
    "Chickweed", "Chicory", "Chive Blossom", "Coltsfoot", "Common Mallow",
    "Common Milkweed", "Common Vetch", "Common Yarrow", "Coneflower",
    "Cow Parsely", "Cowslip", "Crimson Clover", "Crithmum Maritimum",
    "Daisy", "Dandelion", "Fennel", "Firewood", "Gardenia", "Garlic Mustard",
    "Geranium", "Ground Ivy", "Harebell", "Henbit", "Knapweed",
    "Meadowsweet", "Mullein", "Pickerelweed", "Ramsons", "Red Clover"
]

# Print in 3 columns
for i in range(0, len(plants), 3):
    row = plants[i:i+3]
    print(f"  {i+1:2d}. {row[0]:20s}", end="")
    if len(row) > 1:
        print(f"  {i+2:2d}. {row[1]:20s}", end="")
    if len(row) > 2:
        print(f"  {i+3:2d}. {row[2]:20s}")
    else:
        print()

# Model Information
print("\nğŸ¤– PRE-TRAINED MODELS")
print("â”€" * 80)

models_dir = Path('saved_models')
if models_dir.exists():
    models = {
        'best_resnet34.pt': 'ResNet-34',
        'best_googlenet.pt': 'GoogLeNet',
        'best_mobilenetv2.pt': 'MobileNet v2'
    }
    
    for model_file, model_name in models.items():
        model_path = models_dir / model_file
        if model_path.exists():
            size_mb = model_path.stat().st_size / (1024 * 1024)
            print(f"  âœ“ {model_name:15s} - {size_mb:6.2f} MB - {model_file}")
        else:
            print(f"  âœ— {model_name:15s} - Not found")
else:
    print("  âš  Models directory not found")

# Notebooks
print("\nğŸ““ JUPYTER NOTEBOOKS")
print("â”€" * 80)
notebooks = [
    ("1. wep_classifier_initial.ipynb", "Initial model training and evaluation"),
    ("2. wep_classifier_tuning.ipynb", "Hyperparameter tuning and optimization"),
    ("3. visualise_results.ipynb", "Results visualization and comparison")
]

for nb_file, description in notebooks:
    exists = "âœ“" if os.path.exists(nb_file) else "âœ—"
    print(f"  {exists} {nb_file:35s} - {description}")

# Training Details
print("\nâš™ï¸  TRAINING CONFIGURATION")
print("â”€" * 80)
print("""
  â€¢ Epochs: 20
  â€¢ Learning Rate: 0.001
  â€¢ Batch Size: 64
  â€¢ Train/Val/Test Split: 70% / 15% / 15%
  â€¢ Optimizer: Adam
  â€¢ Loss Function: Cross-Entropy
  â€¢ Data Augmentation: Random rotation, flip, crop
  â€¢ Transfer Learning: Pre-trained ImageNet weights
""")

# Performance Metrics
print("\nğŸ“ˆ EVALUATION METRICS")
print("â”€" * 80)
print("""
The models are evaluated using:
  â€¢ Accuracy (Top-1 and Top-5)
  â€¢ Precision, Recall, F1-Score
  â€¢ Confusion Matrix
  â€¢ ROC Curves and AUC
  â€¢ Training/Validation Loss Curves
""")

# How to Use
print("\nğŸš€ HOW TO USE")
print("â”€" * 80)
print("""
1. JupyterLab is running at: http://localhost:8888
   
2. Copy the token from the terminal output when you started JupyterLab

3. Open any of the three notebooks:
   â€¢ Start with notebook 1 for initial training
   â€¢ Use notebook 2 for hyperparameter tuning
   â€¢ View notebook 3 for results visualization

4. Select the 'wep' kernel when prompted

5. Run cells sequentially using Shift+Enter

Note: Training from scratch requires the full dataset (not included in sample).
      The pre-trained models can be used for inference immediately.
""")

# File Structure
print("\nğŸ“ PROJECT STRUCTURE")
print("â”€" * 80)
print("""
wep-classifier/
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ sample/              # Sample images (1 per class)
â”œâ”€â”€ functions/
â”‚   â”œâ”€â”€ model.py            # Classifier architecture
â”‚   â”œâ”€â”€ plotting.py         # Visualization functions
â”‚   â”œâ”€â”€ tuning.py           # Hyperparameter tuning
â”‚   â””â”€â”€ utils.py            # Utility functions
â”œâ”€â”€ saved_models/           # Pre-trained model weights
â”œâ”€â”€ plots/                  # Generated visualizations
â”œâ”€â”€ 1. wep_classifier_initial.ipynb
â”œâ”€â”€ 2. wep_classifier_tuning.ipynb
â””â”€â”€ 3. visualise_results.ipynb
""")

print("=" * 80)
print(" " * 25 + "Setup Complete! ğŸ‰")
print("=" * 80)
print()
